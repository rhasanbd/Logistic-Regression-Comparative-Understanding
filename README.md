If github in unable to render a Jupyter notebook, copy the link of the notebook and enter into the nbviewer:
https://nbviewer.jupyter.org/

## Logistic Regression: A Comparative Understanding

1. Logistic Regression: Binary Classification

      -- In this notebook, we use Scikit-Learn's Gradient Descent algorithm for Logistic Regression to perform binary classification.
      
2. Logistic Regression: Multi-class Classification

      -- In this notebook, we use Scikit-Learn's Gradient Descent algorithm for Logistic Regression to perform multi-class classification. We study two strategies for multi-class classification: One-vesus-all (OvA) & Multinomial (Softmax)
      

3. Logistic Regression: Stochastic Gradient Descent

      -- In this notebook, we apply the Stochastic Gradient Descent (SGD) algorithm for solving a multi-class classification problem using Logistic Regression. We study the impact of two types of regularization: L2/L1 & Early Stopping
      
4. Logistic Regression: Comparison With Other Models

      -- In this notebook, we compare the Logistic Regression classifier model with two other classifier models: K-nearest Neighbors & Naive Bayes classifier.
      
5. Logistic Regression: Optimization Techniques

      -- In this notebook, we learn how to optimize a Logistic Regression classifier. We use a large dataset to reveal the benefit of optimization techniques. We investigate the following two optimization techniques: Optimization Algorithms (solvers) & Dimensionality Reduction
      
      
6. Unconstrained Optimization Algorithms: How To

      -- Scikit-Learn LogisticRegression model uses various algorithms for the optimization in the gradient descent technique. In this notebook we will try to understand how to use some of the optimization algorithms.


      
      
      
