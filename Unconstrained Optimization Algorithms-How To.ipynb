{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "\n",
    "Scikit-Learn LogisticRegression model uses various optimization algorithms for the Gradient Descent algorithm.\n",
    "\n",
    "In this notebook, we try to understand how to use some of the optimization algorithms.\n",
    "\n",
    "\n",
    "\n",
    "## SciPy Optimization \n",
    "\n",
    "The scipy.optimize package provides several commonly used optimization algorithms. \n",
    "https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html\n",
    "\n",
    "We will study the **unconstrained minimization of multivariate scalar functions (minimize)** algorithms.\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\n",
    "\n",
    "The commonly used \"minimize\" algorithms are:\n",
    "- CG (nonlinear conjugate gradient)\n",
    "- Newton-CG (truncated Newton)\n",
    "- TNC (truncated Newton algorithm)\n",
    "- BFGS (Broyden-Fletcher-Goldfarb-Shanno)\n",
    "- L-BFGS-B (bound constrained minimization)\n",
    "- Nelder-Mead\n",
    "- Powell\n",
    "\n",
    "\n",
    "## How do We Use the scipy.optimize.minimize Function?\n",
    "\n",
    "\n",
    "##### scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
    "\n",
    "We will use the folowng arguments:\n",
    "- 1st (fun)\n",
    "- 2nd (x0)\n",
    "- 3rd (args)\n",
    "- 4th (method)\n",
    "- 5th (jac)\n",
    "- 11th (options)\n",
    "\n",
    "\n",
    "See the description below:\n",
    "\n",
    "- 1st(fun):\n",
    "callable\n",
    "The objective function to be minimized. Must be in the form f(x, *args). The optimizing argument, x, is a 1-D array of points, and args is a tuple of any additional fixed parameters needed to completely specify the function.\n",
    "\n",
    "- 2nd (x0):\n",
    "ndarray\n",
    "Initial guess. \n",
    "len(x0) is the dimensionality of the minimization problem.\n",
    "\n",
    "\n",
    "- 3rd (args): \n",
    "tuple, optional\n",
    "Extra arguments passed to the objective function and its derivatives (Jacobian, Hessian).\n",
    "\n",
    "- 4th (method):\n",
    "str or callable, optional\n",
    "\n",
    "Type of solver. Should be one of\n",
    "\n",
    "    ‘Nelder-Mead’ \n",
    "    ‘Powell’ \n",
    "    ‘CG’ \n",
    "    ‘BFGS’ \n",
    "    ‘Newton-CG’ \n",
    "    ‘L-BFGS-B’ \n",
    "    ‘TNC’ \n",
    "    ‘COBYLA’ \n",
    "    ‘SLSQP’ \n",
    "    ‘dogleg’ \n",
    "    ‘trust-ncg’ \n",
    "    ‘trust-exact’ \n",
    "    ‘trust-krylov’ \n",
    "\n",
    "\n",
    "If not given, chosen to be one of BFGS, L-BFGS-B, SLSQP, depending if the problem has constraints or bounds.\n",
    "\n",
    "\n",
    "- 5th (jac):\n",
    "bool or callable, optional\n",
    "\n",
    "Jacobian (gradient) of objective function. Only for CG, BFGS, Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov, trust-region-exact. If jac is a Boolean and is True, fun is assumed to return the gradient along with the objective function. If False, the gradient will be estimated numerically. jac can also be a callable returning the gradient of the objective. In this case, it must accept the same arguments as fun.\n",
    "\n",
    "- 11th (options):\n",
    "dict, optional\n",
    "\n",
    "A dictionary of solver options. All methods accept the following generic options:\n",
    "\n",
    "- maxiter : int\n",
    "Maximum number of iterations to perform.\n",
    "\n",
    "- disp : bool\n",
    "Set to True to print convergence messages.\n",
    "\n",
    "There are other method-specific options.\n",
    "\n",
    "\n",
    "We will see **<font color=red size=4> how to use the BFGS algorithm </font>** by solving three unconstrained optimization problems.\n",
    "\n",
    "\n",
    "## BFGS\n",
    "\n",
    "For the BFGS solver, the following options are available:\n",
    "\t\n",
    "- disp : bool (Set to True to print convergence messages.)\n",
    "- maxiter : int (Maximum number of iterations to perform.)\n",
    "- gtol : float (Gradient norm must be less than gtol before successful termination.)\n",
    "- norm : float [Order of norm (Inf is max, -Inf is min).]\n",
    "- eps : float or ndarray (If jac is approximated, use this value for the step size.)\n",
    "\n",
    "The two most relevant \"options\" for BFGS are: disp and maxiter\n",
    "\n",
    "\n",
    "\n",
    "## Return Values from the \"minimize\" dunction\n",
    "\t\n",
    "res : OptimizeResult\n",
    "\n",
    "The optimization result represented as a OptimizeResult object. \n",
    "\n",
    "Important attributes are: \n",
    "- x: the solution array\n",
    "- success: a Boolean flag indicating if the optimizer exited successfully and message which describes the cause of the termination. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization Problem 1\n",
    "\n",
    "Minimize the following objective function:\n",
    "\n",
    "$f(x) = 4x^2 - 64x$\n",
    "\n",
    "Find the x that minimizes f(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -256.000000\n",
      "         Iterations: 3\n",
      "         Function evaluations: 4\n",
      "         Gradient evaluations: 4\n",
      "\n",
      "Solution:  [8.]\n",
      "Optimizer exits successfully:  True\n"
     ]
    }
   ],
   "source": [
    "# Objectve function to be minimized\n",
    "def f(x):\n",
    "    return 4*x**2 - 64*x\n",
    "\n",
    "\n",
    "# Gradient of the objective function\n",
    "def df(x):\n",
    "    return 8*x - 64\n",
    "\n",
    "\n",
    "# Initial value of x\n",
    "x0 = 1\n",
    "\n",
    "# Try with BFGS\n",
    "result = optimize.minimize(f,x0,method='bfgs',jac=df,options={'disp':True, 'maxiter':20})\n",
    "print(\"\\nSolution: \", result.x)\n",
    "print(\"Optimizer exits successfully: \", result.success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization Problem 2\n",
    "\n",
    "Minimize a general quadratic objective function:\n",
    "\n",
    "A general purely quadratic real function f(x) on $n$ real variables $x_1, ..., x_n$ can always be written as $x^TAx$ where x is the column vector with those variables, and A is a symmetric real matrix ($A = A^T$). \n",
    "\n",
    "$f(x) = x^TAx$\n",
    "\n",
    "Therefore, the matrix (A) being **positive definite** means that f has a **unique minimum (zero)** when x is zero, and is strictly positive for any other x.\n",
    "\n",
    "The determinant of a positive definite matrix is always positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[2. 2.]\n",
      " [2. 3.]]\n",
      "Eigenvalues of A:  [0.43844719 4.56155281]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 4\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "\n",
      "Solution:  [-1.60761179e-07 -1.97082922e-07]\n",
      "Optimizer exits successfully:  True\n"
     ]
    }
   ],
   "source": [
    "# A positive definite matrix to be used in the function definitions\n",
    "A = np.array([[2.,2.],[2.,3.]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "eigenvalues, eigenvectors = LA.eig(A)\n",
    "print(\"Eigenvalues of A: \", eigenvalues)\n",
    "\n",
    "# Objectve function to be minimized: f(x) = x^T A x\n",
    "def f(x):\n",
    "    return np.dot(x.T,np.dot(A,x))\n",
    "\n",
    "# Gradient of the objective function, df = 2*A*x\n",
    "def df(x):\n",
    "    return 2*np.dot(A,x)\n",
    "\n",
    "# Initial value of x\n",
    "x_initial = np.array([1.,2.])\n",
    "\n",
    "# Try with BFGS\n",
    "result = optimize.minimize(fun = f, x0 = x_initial, method = 'bfgs',jac = df,options = {'disp': True, 'maxiter': 20})\n",
    "\n",
    "print(\"\\nSolution: \", result.x)\n",
    "print(\"Optimizer exits successfully: \", result.success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization Problem 3 (Binary Classification using Logistic Regression)\n",
    "\n",
    "Now we will solve a classification problem using Logistic Regression.\n",
    "\n",
    "The goal is to minimize an objective function (cost function) to find optimal model parameters (theta).\n",
    "\n",
    "We will use a synthetic data set that has 4 training examples belonging to 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[10 20 30]\n",
      " [40  1  5]\n",
      " [12 22 32]\n",
      " [38  3  4]]\n",
      "y:\n",
      " [[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 1\n",
      "         Function evaluations: 2\n",
      "         Gradient evaluations: 2\n",
      "\n",
      "Optimal Theta:  [-0.65798587  0.44649041  0.62273663]\n",
      "Optimizer exits successfully:  True\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "\n",
      "\n",
      "Scikit-Learn Logistic Regression\n",
      "\n",
      "No. of Iterations: [13]\n",
      "\n",
      "Weight Intercept: [[-0.17375947  0.11807689  0.16751294]]\n",
      "Weight Coefficients: [[-0.17375947  0.11807689  0.16751294]]\n",
      "\n",
      "Scikit-Learn Confusion Matrix:\n",
      "[[2 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Demo: A simple synthetic dataset is used to understand the implementation of the optimize.minimize solvers \n",
    "#       to find the optimal theta\n",
    "\n",
    "\n",
    "# Note: To use the SciPy minimize function, the objective function must be in the form f(x, *args). \n",
    "# The optimizing argument, x, is a 1-D array of points, \n",
    "#     and args is a tuple of any additional fixed parameters needed to completely specify the function.\n",
    "# In the following \"cost\" and \"grad\" are functions.\n",
    "# The first arguments of these two functions should be \"theta\" (optimization argument)\n",
    "\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "# Cost (cross-entropy) function\n",
    "def cost(theta, X, y):\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "   \n",
    "    # Reshape theta to make it a 1D (n x 1) column vector\n",
    "    # This is important, because \"X\" is a N x d matrix.\n",
    "    # Hence, the dot product cannot be done unless \"theta\" is d x 1\n",
    "    \n",
    "    theta = theta.reshape((d,1))\n",
    "   \n",
    "    h = sigmoid(X.dot(theta))\n",
    "    \n",
    "    J = -(1/N)*np.sum((y*np.log(h) + (1 - y)*np.log(1 - h)))\n",
    "    return J\n",
    "\n",
    "\n",
    "\n",
    "# Gradient of the cost function\n",
    "def grad(theta, X, y):\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    # Reshape theta to make it a 1D (d x 1) column vector\n",
    "    # This is important, because \"X\" is a N x d matrix.\n",
    "    # Hence, the dot product cannot be done unless \"theta\" is d x 1\n",
    "    theta = theta.reshape((d,1));\n",
    "    \n",
    "    h = sigmoid(X.dot(theta))\n",
    "   \n",
    "    grad = (1/N)*(X.T.dot(h - y))\n",
    "    \n",
    "    return grad.flatten()\n",
    "  \n",
    "\n",
    "# Create the data matrix X and target vector y\n",
    "X = np.array([[10, 20, 30],[40, 1, 5], [12, 22, 32],[38, 3, 4]])\n",
    "y = np.array([[1], [0], [1], [0]])\n",
    "\n",
    "print(\"X:\\n\",X)\n",
    "print(\"y:\\n\",y)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize theta\n",
    "theta = np.zeros(X.shape[1]);\n",
    "\n",
    "\n",
    "result = optimize.minimize(fun = cost, x0 = theta, args =(X, y), \n",
    "                           method='bfgs', jac = grad, options={'disp': True, 'maxiter': 100})\n",
    "\n",
    "\n",
    "optimal_theta = result.x;\n",
    "\n",
    "print(\"\\nOptimal Theta: \", optimal_theta)\n",
    "print(\"Optimizer exits successfully: \", result.success)\n",
    "\n",
    "\n",
    "\n",
    "# Predict class probabilities\n",
    "y_proba = sigmoid(X.dot(optimal_theta))\n",
    "\n",
    "\n",
    "# Predict the class labels using class probabilities\n",
    "y_predicted = np.zeros((len(y))).reshape(len(y), 1)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if(y_proba[i] >= 0.5):\n",
    "        y_predicted[i] = 1\n",
    "    else:\n",
    "        y_predicted[i] = 0\n",
    "        \n",
    "        \n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y, y_predicted))\n",
    "\n",
    "\n",
    "print(\"\\n\\nScikit-Learn Logistic Regression\\n\")\n",
    "\n",
    "# Classification using sklearn's Logistic Regression\n",
    "\n",
    "\n",
    "lg_reg_clf = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "y = y.ravel() # LogisticRegression object requires a flattened array, thus we apply the ravel() function\n",
    "\n",
    "lg_reg_clf.fit(X, y)\n",
    "\n",
    "print(\"No. of Iterations:\", lg_reg_clf.n_iter_ )\n",
    "print(\"\\nWeight Intercept:\", lg_reg_clf.coef_ )\n",
    "print(\"Weight Coefficients:\", lg_reg_clf.coef_ )\n",
    "\n",
    "y_predicted = lg_reg_clf.predict(X)\n",
    "\n",
    "print(\"\\nScikit-Learn Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_predicted))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
