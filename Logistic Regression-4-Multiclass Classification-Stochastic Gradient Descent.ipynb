{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Multiclass Classification - Stochastic Gradient Descent\n",
    "\n",
    "In this notebook, we apply the **Stochastic Gradient Descent (SGD)** algorithm for solving a multi-class classification problem using the Logistic Regression model. We study the impact of two types of regularization:\n",
    "\n",
    "- L2/L1\n",
    "- L2/L1 & Early Stopping\n",
    "\n",
    "\n",
    "### <font color=red> Early Stopping</font>\n",
    "\n",
    "Early stopping is a regularization technique for iterative optimization algorithms such as Graient Descent that stops training as soon as the validation error reaches a minimum. \n",
    "\n",
    "Because we start with zero or near zero weights and they move away as training continues, stopping early corresponds to a model with more weights close to zero and effectively fewer parameters.\n",
    "\n",
    "### Early Stopping Curve:\n",
    "\n",
    "With Stochastic (and Mini-batch) Gradient Descent, the error vs iterations (epochs) curves are not so smooth.\n",
    "\n",
    "It may be hard to know whether we have reached the minimum or not. \n",
    "\n",
    "One solution is to stop only after the validation error has been above the minimum for some time (when we are confident that the model will not do any better), then roll back the model parameters to the point where the validation error was at a minimum.\n",
    "\n",
    "More on Stochastic Gradient Descent:\n",
    "https://scikit-learn.org/stable/modules/sgd.html#sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "We will use the iris dataset, which is a multivariate data set. \n",
    "\n",
    "This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica\n",
    "\n",
    "There are 4 features: \n",
    "- sepal length (cm)\n",
    "- sepal width (cm)\n",
    "- petal length (cm)\n",
    "- petal width (cm)\n",
    "\n",
    "Total number of samples: 150\n",
    "\n",
    "The dataset is also known as Fisher's Iris data set as it was introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "\n",
    "\n",
    "<img src=\"https://cse.unl.edu/~hasan/IrisFlowers.png\" width=800, height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Values: \n",
      " ['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']\n",
      "\n",
      "Feature Names: \n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "Target Names: \n",
      " ['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# See the key values\n",
    "print(\"\\nKey Values: \\n\", list(iris.keys()))\n",
    "\n",
    "# The feature names\n",
    "print(\"\\nFeature Names: \\n\", list(iris.feature_names))\n",
    "\n",
    "# The target names\n",
    "print(\"\\nTarget Names: \\n\", list(iris.target_names))\n",
    "\n",
    "# The target values (codes)\n",
    "#print(\"\\nTarget Values: \\n\", list(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix (X) and the Label Array (y)\n",
    "\n",
    "We can use all features or a subset. For this notebook, we will use two features (i.e., petal length, petal width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "(150,)\n",
      "\n",
      "X data type:  float64\n",
      "y data type:  int64\n"
     ]
    }
   ],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"\\nX data type: \", X.dtype)\n",
    "print(\"y data type: \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "The main problem with Batch Gradient Descent is that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. \n",
    "\n",
    "At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance. \n",
    "\n",
    "Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.\n",
    "\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. \n",
    "\n",
    "Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn SGDClassifier\n",
    "\n",
    "\n",
    "The SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.\n",
    "\n",
    "The concrete loss function can be set via the loss parameter. SGDClassifier supports the following loss functions:\n",
    "\n",
    "- loss=\"hinge\": (soft-margin) linear Support Vector Machine\n",
    "- loss=\"modified_huber\": smoothed hinge loss\n",
    "- loss=\"log\": logistic regression\n",
    "\n",
    "For implementing SGD for Logistic Regression, we usually use the \"log\" loss. The \"log\" loss gives logistic regression, a probabilistic classifier.\n",
    "\n",
    "Using loss=\"log\" enables the predict_proba method, which gives a vector of probability estimates per sample.\n",
    "\n",
    "\n",
    "\n",
    "We need to set the following attributes to train a SGDClassifier.\n",
    "\n",
    "\n",
    "- penalty : ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’\n",
    "    -- The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.\n",
    "    \n",
    "\n",
    "- alpha : Constant that multiplies the regularization term. Defaults to 0.0001 \n",
    "\n",
    "\n",
    "- l1_ratio : The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.\n",
    "\n",
    "\n",
    "- max_iter : The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.\n",
    "\n",
    "\n",
    "- tol : The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to 1e-3 from 0.21.\n",
    "\n",
    "\n",
    "- random_state : The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "\n",
    "- learning_rate : The learning rate schedule:\n",
    "\n",
    "    -- ‘constant’: eta = eta0\n",
    "\n",
    "    --‘optimal’: [default] eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "\n",
    "    --‘invscaling’: eta = eta0 / pow(t, power_t)\n",
    "\n",
    "    --‘adaptive’: eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.\n",
    "\n",
    "\n",
    "- eta0 : The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.\n",
    "\n",
    "\n",
    "\n",
    "- early_stopping : Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.\n",
    "\n",
    "\n",
    "- n_iter_no_change : Number of iterations with no improvement to wait before early stopping.\n",
    "\n",
    "More detail: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier for Multi-Class Classification\n",
    "\n",
    "SGDClassifier supports multi-class classification by combining multiple binary classifiers in a **“one versus all” (OvA)** scheme. \n",
    "\n",
    "For each of the classes, a binary classifier is learned that discriminates between that and all other classes. \n",
    "\n",
    "At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. \n",
    "\n",
    "- Note that the Logistic Regression SGDClassifier **does not use the Softmax regression** technique for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of the Stochastic Gradient Descent for Logistic Regression\n",
    "\n",
    "We will investigate the SGDClassifier:\n",
    "- Without Early Stopping\n",
    "- With Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Without Early Stopping\n",
    "\n",
    "We implement the SGDClassifier without early stopping.\n",
    "\n",
    "\n",
    "First, we need to find the optimal hyperparameters via Gridsearch.\n",
    "\n",
    "## Model Selection: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (F1 micro): 0.975000\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 1000, 'penalty': 'l1', 'tol': 1e-08}\n",
      "\n",
      "\n",
      "CPU times: user 1.82 s, sys: 119 ms, total: 1.94 s\n",
      "Wall time: 6.86 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1944 out of 1944 | elapsed:    6.8s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "param_grid = {'alpha': [0.05, 0.01, 0.001],\n",
    "              'penalty' : [\"l2\", \"l1\"],\n",
    "              'learning_rate': [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"], \n",
    "              'max_iter':[500, 1000, 3000],\n",
    "              'eta0': [0.1, 0.01, 0.001],\n",
    "              'tol': [1e-3, 1e-5, 1e-8],\n",
    "              'loss': ['log']}\n",
    "\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "sgd_clf_cv = GridSearchCV(sgd_clf, param_grid, scoring='f1_micro', cv=3, verbose=1, n_jobs=-1)\n",
    "sgd_clf_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = sgd_clf_cv.best_params_\n",
    "\n",
    "print(\"Best Score (F1 micro): %f\" % sgd_clf_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Optimal SGDClassifier without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.01, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l1', power_t=0.5,\n",
       "              random_state=None, shuffle=True, tol=1e-08,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(**params_optimal)\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal SGDClassifier without Early Stopping on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Iterations: 197\n",
      "\n",
      "Weight Coefficients:\n",
      " [[-5.202905   -4.3712365 ]\n",
      " [ 2.50406713 -2.18295529]\n",
      " [ 7.80217266  7.68339677]]\n",
      "\n",
      "Weight Intercept:\n",
      " [ -5.98019404  -0.65560022 -10.59592791]\n",
      "\n",
      "Test Accuracy:  1.0\n",
      "\n",
      "Test Confusion Matrix (Test Data):\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo. of Iterations:\", sgd.n_iter_ )\n",
    "\n",
    "print(\"\\nWeight Coefficients:\\n\", sgd.coef_ )\n",
    "\n",
    "print(\"\\nWeight Intercept:\\n\", sgd.intercept_ )\n",
    "\n",
    "\n",
    "y_test_predict = sgd.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predict == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nTest Confusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent With Early Stopping\n",
    "\n",
    "\n",
    "We implement the Logistic Regression SGDClassifier with early stopping.\n",
    "\n",
    "First, we need to find the optimal hyperparameters via Gridsearch.\n",
    "\n",
    "\n",
    "## Model Selection: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    1.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (F1 Micro): 0.966667\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\n",
      "\n",
      "CPU times: user 1.76 s, sys: 42.9 ms, total: 1.81 s\n",
      "Wall time: 6.73 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1728 out of 1728 | elapsed:    6.7s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "param_grid = {'alpha': [0.05, 0.01, 0.001],\n",
    "              'penalty' : [\"l2\", \"l1\"],\n",
    "              'learning_rate': [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"], \n",
    "              'max_iter':[100, 500, 1000, 3000],\n",
    "              'eta0': [0.01, 0.001],\n",
    "              'tol': [1e-3, 1e-5, 1e-8],\n",
    "              'loss': ['log']}\n",
    "\n",
    "\n",
    "sgd_clf_es = SGDClassifier(early_stopping=True)\n",
    "\n",
    "sgd_clf_es_cv = GridSearchCV(sgd_clf_es, param_grid, scoring='f1_micro', cv=3, verbose=1, n_jobs=-1)\n",
    "sgd_clf_es_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal_es = sgd_clf_es_cv.best_params_\n",
    "\n",
    "print(\"Best Score (F1 Micro): %f\" % sgd_clf_es_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_es)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Optimal SGDClassifier with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.01, average=False, class_weight=None, early_stopping=True,\n",
       "              epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='optimal', loss='log', max_iter=100,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l1', power_t=0.5,\n",
       "              random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_early_stopping = SGDClassifier(early_stopping=True, **params_optimal_es)\n",
    "\n",
    "sgd_early_stopping.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal SGDClassifier with Early Stopping on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Iterations: 6\n",
      "\n",
      "Weight Coefficients:\n",
      " [[-3.39830323 -1.92713383]\n",
      " [ 1.1286852  -1.3544194 ]\n",
      " [ 3.21041892  4.22039655]]\n",
      "\n",
      "Weight Intercept:\n",
      " [-3.15440707 -0.66032816 -4.56737906]\n",
      "\n",
      "Test Accuracy:  0.9666666666666667\n",
      "\n",
      "Test Confusion Matrix (Test Data):\n",
      " [[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo. of Iterations:\", sgd_early_stopping.n_iter_ )\n",
    "\n",
    "print(\"\\nWeight Coefficients:\\n\", sgd_early_stopping.coef_ )\n",
    "\n",
    "print(\"\\nWeight Intercept:\\n\", sgd_early_stopping.intercept_ )\n",
    "\n",
    "y_test_predict = sgd_early_stopping.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predict == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nTest Confusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision: Early Stopping & Without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weight Coefficients (Without Early Stopping):\n",
      " [[-5.202905   -4.3712365 ]\n",
      " [ 2.50406713 -2.18295529]\n",
      " [ 7.80217266  7.68339677]]\n",
      "\n",
      "Weight Coefficients (Early Stopping):\n",
      " [[-3.39830323 -1.92713383]\n",
      " [ 1.1286852  -1.3544194 ]\n",
      " [ 3.21041892  4.22039655]]\n",
      "\n",
      "No. of Iterations (Without Early Stopping): 197\n",
      "No. of Iterations (Early Stopping): 6\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeight Coefficients (Without Early Stopping):\\n\", sgd.coef_ )\n",
    "print(\"\\nWeight Coefficients (Early Stopping):\\n\", sgd_early_stopping.coef_)\n",
    "\n",
    "print(\"\\nNo. of Iterations (Without Early Stopping):\", sgd.n_iter_ )\n",
    "print(\"No. of Iterations (Early Stopping):\", sgd_early_stopping.n_iter_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: Early Stopping Regularization\n",
    "\n",
    "The SGDClassifier (or any gradient descent algorithm) starts with zero or near-zero weights. Then, it moves away as training continues. If we stop the training early, as soon as the validation error reaches minimum, we get a model with **more weights close to zero and effectively fewer parameters**.\n",
    "\n",
    "If we compare the weight coefficients and the number iterations between the two implementations (without and with early stopping), we will see the difference. \n",
    "\n",
    "Early stopping makes the weights smaller (enables feature selection) and requires less iterations (faster training time). \n",
    "\n",
    "However, due to its stochastic nature we will not always get optimal weights. Hence the model performance will not be as good as the batch gradient descent based approach.\n",
    "\n",
    "With smaller dataset (e.g., Iris), we will not see the benefit of SGDclassifier with early stopping. However, for larger datasets (e.g., MNIST handwritten digit recongnition), the SGDClassifier will improve the model performance in two ways:\n",
    "\n",
    "- Faster training time\n",
    "- Feature selection (due to early stopping as well as l2 regularization)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
